{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D_NLP_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ajjc9wtncXuW",
        "SbCIW7GPgNV2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8AVfvSGdtBf",
        "colab_type": "text"
      },
      "source": [
        "### Data setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtxa5eqpalgr",
        "colab_type": "text"
      },
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z343TfthX-Q1",
        "colab_type": "code",
        "outputId": "831ec167-22a7-423a-8f5e-919457c273f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 27.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 2.2MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 3.2MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 2.1MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.6MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 3.1MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 3.5MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 4.0MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 4.5MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 3.5MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 3.5MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 5.0MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 5.0MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 9.6MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 9.6MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 9.6MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 9.6MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 9.7MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 9.8MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 46.1MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 10.3MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 10.3MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 10.4MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 10.4MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 10.5MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 10.2MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 10.2MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 10.1MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 10.0MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 10.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 47.4MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 47.5MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 48.6MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 43.6MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 43.3MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 49.3MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 51.6MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 55.2MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 12.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 12.1MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 12.0MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 12.0MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 12.0MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 12.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 12.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 12.1MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 11.9MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 11.8MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 45.5MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 49.2MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 50.2MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 51.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 50.9MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 54.8MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 55.2MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 56.1MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 59.3MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 60.3MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 58.9MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 63.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 62.5MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 62.0MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 62.5MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 61.2MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 47.3MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 47.1MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 47.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 47.8MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 48.5MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 48.9MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 49.1MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 48.6MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 48.9MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 48.0MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 63.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 64.4MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 63.6MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 65.2MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 64.8MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 13.8MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 13.5MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 13.5MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 13.4MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 13.1MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 13.0MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 13.0MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 13.0MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 13.0MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 13.0MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 50.2MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 53.8MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 53.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 54.3MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 63.4MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 63.9MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 60.8MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 22.5MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqG1H7m_ii8L",
        "colab_type": "code",
        "outputId": "4ca9a39b-9332-4a06-9d64-55c5f5446567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from time import time\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, CuDNNLSTM, CuDNNGRU, Dropout, Bidirectional, Embedding, Flatten, Dense, GlobalMaxPooling1D, Conv1D\n",
        "from keras.initializers import RandomUniform\n",
        "from time import time\n",
        "import keras\n",
        "import math\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from keras.initializers import RandomUniform\n",
        "import math\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "from keras.regularizers import l1, l2, l1_l2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jNiNUw-avU8",
        "colab_type": "text"
      },
      "source": [
        "Loads in data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xx4-yEEiYGM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "glove_size = 300\n",
        "\n",
        "glove_files = {\n",
        "    50: {\"id\": '1WZ74wJa8_Ca5_PwxYalO6_kMrqBwo93i', 'name': 'glove.6B.50d.txt'},\n",
        "    100: {\"id\": '1z540NC9cHtUWDppZUGWQloMTQQbMqJOX', 'name': 'glove.6B.100d.txt'},\n",
        "    200: {\"id\": '1O9R5OsNMAU8IJL7bNprmI0HGzfvwngCQ', 'name': 'glove.6B.200d.txt'},\n",
        "    300: {\"id\": '1rJ9GlJI4QRp6bmGTIk0hMKI-CQNU3osm', 'name': 'glove.6B.300d.txt'}\n",
        "}\n",
        "\n",
        "# downloaded = drive.CreateFile({'id': '1TUfn4whjpplGJMPwva7ePTFX0ELPvcx6'}) \n",
        "# downloaded.GetContentFile('val_30_70_features.csv') \n",
        "# validationData = pd.read_csv('val_30_70_features.csv', escapechar='\\\\', engine='python')\n",
        "\n",
        "dataset_name = 'training'\n",
        "\n",
        "data_files = {\n",
        "    'validation': {\"id\": '1B9GWwTl3UyiMPr6B7klHz2TZxswz60xX', 'name': 'val_30_70.csv'},\n",
        "    'training': {\"id\": '1eIwy9DL__7w15kuxId2kYE4j4Xi-YPs9', 'name': 'training_30_70.csv'},\n",
        "}\n",
        "\n",
        "downloaded = drive.CreateFile(data_files[dataset_name]) \n",
        "downloaded.GetContentFile(data_files[dataset_name]['name']) \n",
        "validationText = pd.read_csv(data_files[dataset_name]['name'], header=0)\n",
        "\n",
        "glovedownload = drive.CreateFile(glove_files[glove_size])\n",
        "glovedownload.GetContentFile(glove_files[glove_size][\"name\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz7M6HxHdCy0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 90% training, 9% test, 1% validation\n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(validationText.loc[:,\"text\"], validationText.loc[:,\"funny\"], test_size=0.1, train_size=0.9, shuffle=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS1b4ST62diR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class WordCounter:\n",
        "    def __init__(self):\n",
        "        self.data = dict()\n",
        "        self.global_id = 0\n",
        "\n",
        "    def add(self, key):\n",
        "        try:\n",
        "            self.data[key][0] += 1\n",
        "        except KeyError:\n",
        "            self.data[key] = list()\n",
        "            self.data[key].append(1)\n",
        "            self.data[key].append(self.global_id)\n",
        "            self.global_id += 1\n",
        "\n",
        "    def get_count(self, key):\n",
        "        try:\n",
        "            val = self.data[key][0]\n",
        "        except KeyError:\n",
        "            val = None\n",
        "        return val\n",
        "      \n",
        "    def get_id(self, key):\n",
        "        try:\n",
        "            val = self.data[key][1]\n",
        "        except KeyError:\n",
        "            val = None\n",
        "        return val\n",
        "\n",
        "    def pop(self, key):\n",
        "        return self.data.pop(key)\n",
        "      \n",
        "    def __len__(self):\n",
        "      return len(self.data.keys())\n",
        "    \n",
        "    def reset_global_ids(self):\n",
        "      self.global_id = 0\n",
        "      for key in self.data.keys():\n",
        "        self.data[key][1] = self.global_id\n",
        "        self.global_id += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4jBRwkAT8Zr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_in_place(data, word_count=None, trim_tail=True, vect_len=100, lower_case=True):\n",
        "    if word_count is None:\n",
        "        word_count = WordCounter()\n",
        "        for row in data:\n",
        "          words = nltk.word_tokenize(row)\n",
        "          if len(words) > vect_len:\n",
        "            words = words[:vect_len]\n",
        "          for w in words:\n",
        "              if lower_case:\n",
        "                w = w.lower()\n",
        "              word_count.add(w)\n",
        "            \n",
        "    processed = list()\n",
        "    for row in data:\n",
        "      words = nltk.word_tokenize(row)\n",
        "      if len(words) > vect_len:\n",
        "          words = words[:vect_len]\n",
        "      wc = 0\n",
        "      for w in words:\n",
        "          if lower_case:\n",
        "            w = w.lower()\n",
        "          count = word_count.get_count(w)\n",
        "          if count is None:\n",
        "              words[wc] = \"<unk>\"\n",
        "          elif trim_tail and count == 1:\n",
        "              words[wc] = \"<unk>\"\n",
        "              word_count.pop(w)\n",
        "              word_count.add(\"<unk>\")\n",
        "          wc += 1\n",
        "      processed.append(words)\n",
        "    word_count.reset_global_ids()\n",
        "    return processed, word_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCkN965257q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def process_data(data, wc=None, trim_tail=True, vect_len=100):\n",
        "    message = \"\"\n",
        "    data_proc, data_wc = process_in_place(data, wc, trim_tail=trim_tail)\n",
        "    print(\"Size of vocabulary: %d\" % len(data_wc))\n",
        "    print(\"Amount of unkown words: \" + str(data_wc.get_count(\"<unk>\")))\n",
        "    sequences = list()\n",
        "    for line in data_proc:\n",
        "        sequence = list()\n",
        "        for w in line:\n",
        "          id = data_wc.get_id(w)\n",
        "          if id is None:\n",
        "             id = data_wc.get_id(\"<unk>\")\n",
        "          sequence.append(id)\n",
        "        sequences.append(sequence)\n",
        "#     print(sequence)\n",
        "    X = pad_sequences(sequences, maxlen=vect_len, padding='post')\n",
        "    return X, data_wc, data_proc\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_fGwUUN-UyN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def measure(model, test_data, test_labels):\n",
        "    metrics = model.evaluate(test_data, test_labels)\n",
        "    for i in range(len(metrics)):\n",
        "        print(str(model.metrics_names[i] + \" = %f\" % metrics[i]))\n",
        "    print(str(\"perplexity = \" + str(math.exp(metrics[0]))))\n",
        "    \n",
        "    predictions = model.predict_classes(test_data)\n",
        "    f1 = sklearn.metrics.f1_score(test_labels, predictions)\n",
        "    print('F1:', f1)\n",
        "    ba = sklearn.metrics.balanced_accuracy_score(test_labels, predictions)\n",
        "    print(\"Bal Acc:\", ba)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLuF0WXa-UZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Improved upon from https://josephguhlin.com/importing-glove-embeddings-into-tensorflow/\n",
        "def dawn_glove(wc, embedding_dim = 50):\n",
        "    with open(\"glove.6B.\"+str(embedding_dim)+\"d.txt\", 'r') as f:\n",
        "      vectors = {}\n",
        "      for line in f:\n",
        "          vals = line.rstrip().split(' ')\n",
        "          if wc.get_id(vals[0]) is not None:\n",
        "              vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
        "\n",
        "    vocab_size = len(vectors.keys())\n",
        "\n",
        "    vocab = {w: idx for idx, w in enumerate(vectors.keys())}\n",
        "    vector_dim = len(vectors[\"the\"])\n",
        "    W = np.zeros((vocab_size, vector_dim))\n",
        "    for word, v in vectors.items():\n",
        "      if word == '':\n",
        "          continue\n",
        "      W[vocab[word], :] = v\n",
        "\n",
        "    # normalize each word vector to unit variance\n",
        "    W_norm = np.zeros(W.shape)\n",
        "    d = (np.sum(W ** 2, 1) ** (0.5))\n",
        "\n",
        "    W_norm = (W.T / d).T\n",
        "    return W_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKtZpo9zJuXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Used from https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric.\n",
        "\n",
        "    Only computes a batch-wise average of recall.\n",
        "\n",
        "    Computes the recall, a metric for multi-label classification of\n",
        "    how many relevant items are selected.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric.\n",
        "\n",
        "    Only computes a batch-wise average of precision.\n",
        "\n",
        "    Computes the precision, a metric for multi-label classification of\n",
        "    how many selected items are relevant.\n",
        "    \"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "  \n",
        "def specificity(y_true, y_pred):\n",
        "    \"\"\"TN/N\"\"\"\n",
        "    tn = tf.metrics.true_negatives(y_true, y_pred)\n",
        "    fp = tf.metrics.false_positives(y_true, y_pred)\n",
        "    n = tf.math.add(tn, fp)\n",
        "    result = tf.math.divide(tn, n)\n",
        "    return result\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "#     return sklearn.metrics.f1_score(y_true, y_pred)\n",
        "    prec = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prec*rec)/(prec+rec+K.epsilon()))\n",
        "\n",
        "def bal_acc(y_true, y_pred):\n",
        "    r = recall(y_true, y_pred)\n",
        "    s = specificity(y_true, y_pred)\n",
        "#     s = K.metrics.specificity_at_sensitivity(y_true, y_pred, 0)\n",
        "    return K.math.divide(tf.math.add(r, s), 2)\n",
        "#     return sklearn.metrics.balanced_accuracy_score(list(y_true), list(y_pred))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpu_f9GolI9v",
        "colab_type": "code",
        "outputId": "fa77058d-4511-44a1-ba6d-f4ae176ede1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "sent_len = 150\n",
        "doc_amount = len(x_train)\n",
        "# doc_amount = 100000\n",
        "# Isolated this because it takes a long time\n",
        "X, data_wc, data_proc = process_data(x_train[:doc_amount].values, wc=None, trim_tail=True, vect_len=sent_len)\n",
        "test_X, test_data_wc, test_data_proc = process_data(x_test[:doc_amount].values, wc=data_wc, trim_tail=True, vect_len=sent_len)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of vocabulary: 122311\n",
            "Amount of unkown words: 257900\n",
            "Size of vocabulary: 122311\n",
            "Amount of unkown words: 257900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9BhjTgldf44",
        "colab_type": "text"
      },
      "source": [
        "### FFNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thMsoYbzbq1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_FFNN(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "  start_time = time()\n",
        "#   print(init.shape)\n",
        "  init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(data_wc), 300, input_length=vect_len, embeddings_initializer=init))\n",
        "#   model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(500, activation=tf.nn.relu))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(100, activation=tf.nn.relu))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1, activation=tf.sigmoid))\n",
        "  print(str(model.summary()))\n",
        "  # compile network\n",
        "  learningRate = 0.00001\n",
        "  decay = learningRate/epochs\n",
        "  adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "  model.compile(loss=loss, optimizer=adamOpt, metrics=[f1])\n",
        "  # fit network\n",
        "  model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "  print(str(\"runtime: \" + str(time() - start_time)))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBVsSfcPsLN4",
        "colab_type": "code",
        "outputId": "7ac41542-2885-4218-a460-bed8d9365af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1380
        }
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "model2 = learn_FFNN(X, y_train[:doc_amount].values, epochs=20, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "measure(model2, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 150, 300)          36789300  \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 45000)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 500)               22500500  \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               50100     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 59,340,001\n",
            "Trainable params: 59,340,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 492538 samples, validate on 54727 samples\n",
            "Epoch 1/20\n",
            " - 700s - loss: 0.5096 - f1: 0.4446 - val_loss: 0.4916 - val_f1: 0.5347\n",
            "Epoch 2/20\n",
            " - 694s - loss: 0.4796 - f1: 0.5447 - val_loss: 0.4842 - val_f1: 0.5641\n",
            "Epoch 3/20\n",
            " - 697s - loss: 0.4620 - f1: 0.5771 - val_loss: 0.4840 - val_f1: 0.5327\n",
            "Epoch 4/20\n",
            " - 694s - loss: 0.4417 - f1: 0.6072 - val_loss: 0.4911 - val_f1: 0.5438\n",
            "Epoch 5/20\n",
            " - 694s - loss: 0.4172 - f1: 0.6442 - val_loss: 0.5051 - val_f1: 0.5441\n",
            "Epoch 6/20\n",
            " - 696s - loss: 0.3847 - f1: 0.6876 - val_loss: 0.5319 - val_f1: 0.4960\n",
            "Epoch 7/20\n",
            " - 694s - loss: 0.3457 - f1: 0.7338 - val_loss: 0.5647 - val_f1: 0.5452\n",
            "Epoch 8/20\n",
            " - 695s - loss: 0.3005 - f1: 0.7828 - val_loss: 0.6019 - val_f1: 0.5200\n",
            "Epoch 9/20\n",
            " - 695s - loss: 0.2481 - f1: 0.8344 - val_loss: 0.6684 - val_f1: 0.5156\n",
            "Epoch 10/20\n",
            " - 697s - loss: 0.1935 - f1: 0.8817 - val_loss: 0.7650 - val_f1: 0.5213\n",
            "Epoch 11/20\n",
            " - 696s - loss: 0.1418 - f1: 0.9208 - val_loss: 0.8637 - val_f1: 0.4992\n",
            "Epoch 12/20\n",
            " - 696s - loss: 0.0985 - f1: 0.9495 - val_loss: 1.0199 - val_f1: 0.4994\n",
            "Epoch 13/20\n",
            " - 696s - loss: 0.0675 - f1: 0.9672 - val_loss: 1.1448 - val_f1: 0.5001\n",
            "Epoch 14/20\n",
            " - 696s - loss: 0.0462 - f1: 0.9785 - val_loss: 1.2923 - val_f1: 0.5060\n",
            "Epoch 15/20\n",
            " - 695s - loss: 0.0319 - f1: 0.9852 - val_loss: 1.4485 - val_f1: 0.5031\n",
            "Epoch 16/20\n",
            " - 695s - loss: 0.0224 - f1: 0.9903 - val_loss: 1.5733 - val_f1: 0.4959\n",
            "Epoch 17/20\n",
            " - 698s - loss: 0.0159 - f1: 0.9932 - val_loss: 1.6494 - val_f1: 0.4908\n",
            "Epoch 18/20\n",
            " - 697s - loss: 0.0115 - f1: 0.9951 - val_loss: 1.7403 - val_f1: 0.5005\n",
            "Epoch 19/20\n",
            " - 698s - loss: 0.0083 - f1: 0.9966 - val_loss: 1.8588 - val_f1: 0.4848\n",
            "Epoch 20/20\n",
            " - 698s - loss: 0.0066 - f1: 0.9972 - val_loss: 1.9154 - val_f1: 0.4871\n",
            "runtime: 13924.501128196716\n",
            "60808/60808 [==============================] - 3s 51us/step\n",
            "loss = 1.876902\n",
            "f1 = 0.486642\n",
            "perplexity = 6.53323045746447\n",
            "F1: 0.5015339233038348\n",
            "Bal Acc: 0.6488890158929737\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH4ZQacHo8Ln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model10 = learn_FFNN(X, y_train[:doc_amount].values, epochs=25, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(model10, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajjc9wtncXuW",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ho08vyC5yiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adapted from https://machinelearningmastery.com/develop-word-based-neural-language-models-python-keras/\n",
        "def learn_LSTM(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "  start_time = time()\n",
        "  print(init.shape)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(CuDNNLSTM(model_size))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(vect_len, activation=tf.sigmoid))\n",
        "  model.add(Dense(1, activation=tf.sigmoid))\n",
        "  print(str(model.summary()))\n",
        "  # compile network\n",
        "  model.compile(loss=loss, optimizer='adam', metrics=['accuracy'])\n",
        "  # fit network\n",
        "  model.fit(train_data, train_labels, epochs=epochs)\n",
        "  print(str(\"runtime: \" + str(time() - start_time)))\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdzzgZgvnQnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "init = dawn_glove(data_wc, glove_size)\n",
        "model2 = learn_LSTM(X, y_train[:doc_amount].values, epochs=20, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "measure(model3, test_X, y_test[:doc_amount].values)\n",
        "model3 = learn_LSTM(X, y_train[:doc_amount].values, epochs=20, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=True)\n",
        "measure(model4, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpT4N0-xcjEe",
        "colab_type": "text"
      },
      "source": [
        "### BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrSfCXqtTfLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_BiLSTM(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "  start_time = time()\n",
        "  init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(len(data_wc), 300, input_length=vect_len, embeddings_initializer=init))\n",
        "#   model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "  model.add(Bidirectional(CuDNNLSTM(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Bidirectional(CuDNNLSTM(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(100, activation=tf.nn.relu))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation=tf.sigmoid))\n",
        "  print(str(model.summary()))\n",
        "  # compile network\n",
        "  learningRate = 0.00001\n",
        "  decay = learningRate/epochs\n",
        "  adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "  model.compile(loss=loss, optimizer='adam', metrics=[f1])\n",
        "  # fit network\n",
        "  model.fit(train_data, train_labels, epochs=epochs, validation_split=0.05, verbose=2)\n",
        "  print(str(\"runtime: \" + str(time() - start_time)))\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tu5Qac7CVt23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model4 = learn_BiLSTM(X, y_train[:doc_amount].values, epochs=40, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(model4, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8T7TBs1782f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model11 = learn_BiLSTM(X, y_train[:doc_amount].values, epochs=25, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(model11  , test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cmXJOFBh4y8",
        "colab_type": "code",
        "outputId": "d2e33316-3b9a-4231-daf2-a150d1c836bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        }
      },
      "source": [
        "init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "# init = dawn_glove(data_wc, glove_size)\n",
        "model30 = learn_BiLSTM(X, y_train[:doc_amount].values, epochs=1, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "measure(model30, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 150, 300)          36789300  \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 150, 200)          321600    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 150, 200)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 150, 200)          241600    \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               3000100   \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 40,352,701\n",
            "Trainable params: 40,352,701\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 519901 samples, validate on 27364 samples\n",
            "Epoch 1/1\n",
            " - 1098s - loss: 0.4866 - f1: 0.5153 - val_loss: 0.4750 - val_f1: 0.4946\n",
            "runtime: 1102.0843329429626\n",
            "60808/60808 [==============================] - 33s 546us/step\n",
            "loss = 0.472629\n",
            "f1 = 0.495332\n",
            "perplexity = 1.6042061638406775\n",
            "F1: 0.5157667085251849\n",
            "Bal Acc: 0.6670067870382206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h8cNz-Kcr9q",
        "colab_type": "text"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14RmoIV-GgRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ideas from https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
        "def learn_CNN(train_data, train_labels, epochs=10, embed_size=100, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True, kgram=2):\n",
        "    init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(data_wc), 300, input_length=vect_len, embeddings_initializer=init))\n",
        "#     model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "    model.add(Conv1D(filters=100, kernel_size=kgram, padding='valid', activation='relu', strides=1))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(50 ,activation=tf.nn.relu))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    print(str(model.summary()))\n",
        "    learningRate = 0.001\n",
        "    decay = learningRate/epochs\n",
        "    adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "    model.compile(loss=loss, optimizer=adamOpt, metrics=[f1])\n",
        "    model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScdKtbPKOnWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model23 = learn_CNN(X, y_train[:doc_amount].values, epochs=15, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False, kgram=2)\n",
        "# measure(model23, test_X, y_test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCzdZ71RHKs6",
        "colab_type": "code",
        "outputId": "a54eb16f-890d-4366-f50c-00d272060eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "model5 = learn_CNN(X, y_train[:doc_amount].values, epochs=15, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False, kgram=3)\n",
        "measure(model5, test_X, y_test.values)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 150, 300)          36766800  \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 148, 100)          90100     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 148, 100)          0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 50)                5050      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 36,862,001\n",
            "Trainable params: 36,862,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 492538 samples, validate on 54727 samples\n",
            "Epoch 1/15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTTR-ms4Nkau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model22 = learn_CNN(X, y_train[:doc_amount].values, epochs=10, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False, kgram=4)\n",
        "# measure(model22, test_X, y_test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMYzSicfcuh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model22 = learn_CNN(X, y_train[:doc_amount].values, epochs=10, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False, kgram=10)\n",
        "# measure(model22, test_X, y_test.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP_8xmGCfant",
        "colab_type": "text"
      },
      "source": [
        "###BiGRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgiTOfqtfdk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_BiGRU(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "  start_time = time()\n",
        "  # define model\n",
        "#   print(init.shape)\n",
        "# init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "  model = Sequential()\n",
        "#   model.add(Embedding(len(data_wc), 300, input_length=vect_len, embeddings_initializer=init))\n",
        "  model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "  model.add(Bidirectional(CuDNNGRU(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Bidirectional(CuDNNGRU(model_size), input_shape=(vect_len, 1)))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(100, activation=tf.nn.relu))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1, activation=tf.sigmoid))\n",
        "  print(str(model.summary()))\n",
        "  # compile network\n",
        "  learningRate = 0.001\n",
        "  decay = learningRate/epochs\n",
        "  adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "  model.compile(loss=loss, optimizer=adamOpt, metrics=[f1])\n",
        "  # fit network\n",
        "  model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "  print(str(\"runtime: \" + str(time() - start_time)))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2C0HyzoBMxZ",
        "colab_type": "code",
        "outputId": "784a5c7d-a5c7-4f19-e9b8-1fb4bb0c7ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "init = dawn_glove(data_wc, glove_size)\n",
        "model20 = learn_BiGRU(X, y_train[:doc_amount].values, epochs=5, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "predictions = model20.predict_classes(test_X)\n",
        "measure(model20, test_X, y_test[:doc_amount].values)\n",
        "confusionMatrix = sklearn.metrics.confusion_matrix(predictions, y_test[:doc_amount].values)\n",
        "print('Confusion Matrix:', confusionMatrix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 150, 300)          19426500  \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 150, 200)          241200    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 150, 200)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 200)               181200    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 19,869,101\n",
            "Trainable params: 442,601\n",
            "Non-trainable params: 19,426,500\n",
            "_________________________________________________________________\n",
            "None\n",
            "Train on 492538 samples, validate on 54727 samples\n",
            "Epoch 1/5\n",
            " - 503s - loss: 0.5116 - f1: 0.4407 - val_loss: 0.4931 - val_f1: 0.4707\n",
            "Epoch 2/5\n",
            " - 501s - loss: 0.4920 - f1: 0.4996 - val_loss: 0.4873 - val_f1: 0.4849\n",
            "Epoch 3/5\n",
            " - 502s - loss: 0.4868 - f1: 0.5123 - val_loss: 0.4846 - val_f1: 0.5212\n",
            "Epoch 4/5\n",
            " - 502s - loss: 0.4837 - f1: 0.5206 - val_loss: 0.4893 - val_f1: 0.4347\n",
            "Epoch 5/5\n",
            " - 502s - loss: 0.4813 - f1: 0.5255 - val_loss: 0.4829 - val_f1: 0.5018\n",
            "runtime: 2512.1343154907227\n",
            "60808/60808 [==============================] - 30s 485us/step\n",
            "loss = 0.486200\n",
            "f1 = 0.506673\n",
            "perplexity = 1.6261258921325554\n",
            "F1: 0.5218424962852897\n",
            "Bal Acc: 0.6678291132683034\n",
            "Confusion Matrix: [[38425 10439]\n",
            " [ 4042  7902]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kk9IEj9fxoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# model21 = learn_BiGRU(X, y_train[:doc_amount].values, epochs=10, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(model21, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0KJdvfn1dsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "# init = RandomUniform(minval=-0.05, maxval=0.05, seed=None)\n",
        "# model40 = learn_BiGRU(X, y_train[:doc_amount].values, epochs=5, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(model40, test_X, y_test[:doc_amount].values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z8i1Kv0_-76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model40)\n",
        "\n",
        "\n",
        "predictions = model40.predict_classes(test_X[10:20])\n",
        "labels = y_test.values[10:20]\n",
        "text_data = x_test[10:20]\n",
        "\n",
        "print(predictions)\n",
        "print(labels)\n",
        "\n",
        "print(predictions[2:3])\n",
        "print(labels[2:3])\n",
        "print(text_data[2:3].values)\n",
        "\n",
        "# print('False negative:', x_test[18:19])\n",
        "\n",
        "# print(text_data)\n",
        "\n",
        "# print('True negative:', x_test.iloc[0])\n",
        "# print('True negative:', x_test.iloc[1])\n",
        "# print('True positive:', x_test.iloc[2])\n",
        "# print('False positive:', x_test.iloc[3])\n",
        "# print('True positive:', x_test.iloc[6])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbCIW7GPgNV2",
        "colab_type": "text"
      },
      "source": [
        "### Ensemble - BiGRU, BiLSTM, CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKmnsvNOgXNo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def learn_BiLSTM(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "#   start_time = time()\n",
        "#   print(init.shape)\n",
        "#   model = Sequential()\n",
        "#   model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "#   model.add(Bidirectional(CuDNNLSTM(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Bidirectional(CuDNNLSTM(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "#   model.add(Flatten())\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(100, activation=tf.nn.relu))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(1, activation=tf.sigmoid))\n",
        "#   print(str(model.summary()))\n",
        "#   # compile network\n",
        "#   learningRate = 0.00001\n",
        "#   decay = learningRate/epochs\n",
        "#   adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "#   model.compile(loss=loss, optimizer='adam', metrics=[f1])\n",
        "#   # fit network\n",
        "#   model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "#   print(str(\"runtime: \" + str(time() - start_time)))\n",
        "#   return model\n",
        "# # Ideas from https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74\n",
        "# def learn_CNN(train_data, train_labels, epochs=10, embed_size=100, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True, kgram=2):\n",
        "#     model = Sequential()\n",
        "#     model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "#     model.add(Conv1D(filters=100, kernel_size=kgram, padding='valid', activation='relu', strides=1))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(GlobalMaxPooling1D())\n",
        "#     model.add(Dense(50 ,activation=tf.nn.relu))\n",
        "#     model.add(Dropout(0.2))\n",
        "#     model.add(Dense(1, activation='sigmoid'))\n",
        "#     learningRate = 0.001\n",
        "#     decay = learningRate/epochs\n",
        "#     adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "#     model.compile(loss=loss, optimizer=adamOpt, metrics=[f1])\n",
        "#     model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "#     return model\n",
        "# def learn_BiGRU(train_data, train_labels, epochs=10, model_size=100, vect_len=100, loss='binary_crossentropy', weights=None, train_embedding=True):\n",
        "#   start_time = time()\n",
        "#   # define model\n",
        "#   print(init.shape)\n",
        "#   model = Sequential()\n",
        "#   model.add(Embedding(init.shape[0], init.shape[1], input_length=vect_len, weights=[weights], trainable=train_embedding))\n",
        "#   model.add(Bidirectional(CuDNNGRU(model_size, return_sequences=True), input_shape=(vect_len, 1)))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Bidirectional(CuDNNGRU(model_size), input_shape=(vect_len, 1)))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(100, activation=tf.nn.relu))\n",
        "#   model.add(Dropout(0.2))\n",
        "#   model.add(Dense(1, activation=tf.sigmoid))\n",
        "#   print(str(model.summary()))\n",
        "#   # compile network\n",
        "#   learningRate = 0.001\n",
        "#   decay = learningRate/epochs\n",
        "#   adamOpt = keras.optimizers.Adam(lr=learningRate, decay=decay)\n",
        "#   model.compile(loss=loss, optimizer=adamOpt, metrics=[f1])\n",
        "#   # fit network\n",
        "#   model.fit(train_data, train_labels, epochs=epochs, validation_split=0.1, verbose=2)\n",
        "#   print(str(\"runtime: \" + str(time() - start_time)))\n",
        "#   return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0pgnTO8hN2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init = dawn_glove(data_wc, glove_size)\n",
        "\n",
        "# BiLSTMModel = learn_BiLSTM(X, y_train[:doc_amount].values, epochs=4, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(BiLSTMModel, test_X, y_test.values)\n",
        "# BiGRUModel = learn_BiGRU(X, y_train[:doc_amount].values, epochs=10, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False)\n",
        "# measure(BiGRUModel, test_X, y_test.values)\n",
        "# CNNModel = learn_CNN(X, y_train[:doc_amount].values, epochs=1, model_size=100, vect_len=sent_len, loss='binary_crossentropy', weights=init, train_embedding=False, kgram=3)\n",
        "# measure(CNNModel, test_X, y_test.values)\n",
        "\n",
        "\n",
        "# combo = np.append(BiLSTMModel.predict_classes(test_X), BiGRUModel.predict_classes(test_X), axis=1)\n",
        "# combo = np.append(combo, CNNModel.predict_classes(test_X), axis=1)\n",
        "# modePredictions = pd.DataFrame(combo).mode(axis='columns')\n",
        "# ensembleF1 = sklearn.metrics.f1_score(modePredictions.values, y_test[:doc_amount].values)\n",
        "# print('Ensemble F1 Score:', str(ensembleF1))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}